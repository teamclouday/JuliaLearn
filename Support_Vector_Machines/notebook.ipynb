{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "[Reference 1](https://scikit-learn.org/stable/modules/svm.html)  \n",
    "[Reference 2](https://en.wikipedia.org/wiki/Support_vector_machine)  \n",
    "[Reference 3](http://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM classification problem (directly taken from sklearn website):  \n",
    "\n",
    "Given training vectors $x_i\\in \\mathbb{R}^P$, $i=1,...,n$, and a vector $y\\in\\{1,-1\\}^n$  \n",
    "The goal is to find $w\\in \\mathbb{R}^P$ and $b\\in \\mathbb{R}$ such that prediction given by $\\text{sign}(w^T\\phi(x)+b)$ is accurate for most samples  \n",
    "\n",
    "Solve the following __primal problem__:  \n",
    "$$\\begin{align}\n",
    "\\min_{w,b,\\zeta}\\frac{1}{2}w^Tw+C\\sum^n_{i=1}\\zeta_i\\\\\n",
    "\\text{subject to }y_i(w^T\\phi(x_i)+b)\\geq1-\\zeta_i,\\\\\n",
    "\\zeta_i\\geq0,i=1,...,n\n",
    "\\end{align}$$\n",
    "\n",
    "Intuition is we are maximizing the margin (by minimizing $\\|w\\|^2=w^Tw$), while penalize when a sample is misclassified\n",
    "\n",
    "The __dual problem__ to primal is: \n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_\\alpha\\frac{1}{2}\\alpha^TQ\\alpha-e^T\\alpha\\\\\n",
    "\\text{subject to }y^T\\alpha=0,\\\\\n",
    "0\\leq\\alpha_i\\leq C,i=1,...,n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$e$ is vector of all ones, $Q$ is nxn positive semidefinite matrix, $Q_{ij}\\equiv y_iy_jK(x_i,x_j)$, and $K(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)$ is the kernel  \n",
    "$\\alpha_i$ are dual coefficients, upper-bounded by $C$  \n",
    "Highlighting the fact that training vectors are implicitly mapped to a higher (or infinite) dimensional space by function $\\phi$  \n",
    "\n",
    "After optimization problem solved, the __output__ for a given sample $x$ is:\n",
    "$$\\sum_{i\\in SV}y_i\\alpha_iK(x_i,x)+b$$\n",
    "\n",
    "\n",
    "The problem solved by `liblinear` for `LinearSVC` is a equivant form of primal problem:\n",
    "$$\\min_{w,b}\\frac{1}{2}w^Tw+C\\sum_{i=1}\\max(0,y_i(w^T\\phi(x_i)+b))$$\n",
    "which does not involve inner products between samples, and therefore cannot apply kernel tricks  \n",
    "\n",
    "$$C=\\frac{1}{\\text{alpha}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Random\n",
    "import Statistics\n",
    "import LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../tools.jl\")\n",
    "import .JuTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM Implementation (Hinge Loss)  \n",
    "[Reference](https://stackoverflow.com/questions/48804198/soft-margin-in-linear-support-vector-machine-using-python)  \n",
    "\n",
    "Cost function is:\n",
    "$$J=\\frac{1}{2}w^Tw+\\frac{C}{N}\\sum^N_{i=1}\\max\\Big(0,1-y_i(w^T\\phi(x_i)+b)\\Big)$$\n",
    "\n",
    "Gradient function for $w$ is:\n",
    "$$\\frac{\\partial J}{\\partial w}=w + \\frac{C}{N}\\sum^N_{i=1}\\begin{cases}\n",
    "0 & y_i(w^T\\phi(x_i)+b) \\geq 1\\\\\n",
    "-y_i\\phi(x_i) & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "Gradient function for $b$ is:\n",
    "$$\\frac{\\partial J}{\\partial b}=\\frac{C}{N}\\sum^N_{i=1}\\begin{cases}\n",
    "0 & y_i(w^T\\phi(x_i)+b) \\geq 1\\\\\n",
    "-y_i & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a struct to store weights\n",
    "# this should be returned by a training function\n",
    "# alpha should be treated as constant\n",
    "mutable struct WeightsLinearSVM\n",
    "    C::AbstractFloat\n",
    "    w::Array{T} where T<:AbstractFloat\n",
    "    b::AbstractFloat\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cost (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define cost function for linear SVM\n",
    "# assum Y_data is {-1, 1}\n",
    "function cost(X_data::Array{T} where T<:Number, Y_data::Array{T} where T<:Number, weights::WeightsLinearSVM)::AbstractFloat\n",
    "    @assert ndims(Y_data) == ndims(weights.w) == 1\n",
    "    @assert size(X_data) == (size(Y_data)[1], size(weights.w)[1])\n",
    "    loss_w = 0.5 * (weights.w' * weights.w)\n",
    "    loss_inner = 1.0 .- Y_data .* vec(X_data * weights.w .+ weights.b)\n",
    "    loss_inner .= map(m->max(0.0,m), loss_inner)\n",
    "    loss = loss_w + weights.C * sum(loss_inner) / size(X_data)[1]\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 2)\n",
      "(300, 2)\n",
      "(700,)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "X_data, Y_data = JuTools.data_generate_linear_2d()\n",
    "Y_data .= Y_data .* 2.0 .- 1.0 # convert from {0,1} to {-1,1}\n",
    "X_train, X_test, Y_train, Y_test = JuTools.split_data(X_data, Y_data)\n",
    "println(size(X_train))\n",
    "println(size(X_test))\n",
    "println(size(Y_train))\n",
    "println(size(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.320976290385327"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_test = WeightsLinearSVM(1.0, Random.randn(size(X_data)[2]), Random.randn())\n",
    "cost(X_data, Y_data, weight_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learn! (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the learning function (gradient descent)\n",
    "function learn!(X_data::Array{T} where T<:Number, Y_data::Array{T} where T<:Number, weights::WeightsLinearSVM, alpha::AbstractFloat)\n",
    "    @assert ndims(Y_data) == ndims(weights.w) == 1\n",
    "    @assert size(X_data) == (size(Y_data)[1], size(weights.w)[1])\n",
    "    # compute deciding feature\n",
    "    decide = (Y_data .* (X_data * weights.w .+ weights.b)) .< 1 # (? < 1) will be 1, otherwise 0\n",
    "    # update w\n",
    "    gradient_w = weights.w .+ (weights.C / size(X_data)[1]) .* vec(-(Y_data .* decide)' * X_data)\n",
    "    gradient_w .= gradient_w .* alpha\n",
    "    weights.w .= weights.w .- gradient_w\n",
    "    # update b\n",
    "    gradient_b = (weights.C / size(X_data)[1]) * sum(-(Y_data .* decide))\n",
    "    gradient_b *= alpha\n",
    "    weights.b = weights.b - gradient_b\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define prediction function\n",
    "function predict_proba(X_predict::Array{T} where T<:Number, weights::WeightsLinearSVM)::Array\n",
    "    @assert ndims(X_predict) == 2\n",
    "    @assert size(X_predict)[2] == size(weights.w)[1]\n",
    "    prediction = vec(X_predict * weights.w .+ weights.b)\n",
    "    return prediction\n",
    "end\n",
    "\n",
    "# output prediction is in {-1, 1}\n",
    "function predict(X_predict::Array{T} where T<:Number, weights::WeightsLinearSVM)::Array\n",
    "    @assert ndims(X_predict) == 2\n",
    "    @assert size(X_predict)[2] == size(weights.w)[1]\n",
    "    prediction = vec(X_predict * weights.w .+ weights.b)\n",
    "    prediction .= map(m -> m >= 0 ? 1.0 : -1.0, prediction)\n",
    "    return prediction\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_linear (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training function for linear SVM\n",
    "# assume Y_data is in {-1, 1}\n",
    "# this function is similar to the training function for Logistic Regression (Both are gradient descent)\n",
    "function train_linear(X_data::Array{T} where T<:Number, Y_data::Array{T} where T<:Number, C::AbstractFloat;\n",
    "        learning_rate::AbstractFloat=0.1, max_iter::Integer=1000, n_iter_no_change::Integer=5, tol::AbstractFloat=0.001,\n",
    "        verbose::Bool=false, shuffle::Bool=true, early_stop::Bool=true)::WeightsLinearSVM\n",
    "    @assert ndims(X_data) == ndims(Y_data) + 1 == 2\n",
    "    @assert size(X_data)[1] == size(Y_data)[1]\n",
    "    @assert max_iter >= 0\n",
    "    @assert n_iter_no_change >= 0\n",
    "    @assert tol >= 0\n",
    "    X_data = Float64.(X_data)\n",
    "    Y_data = Float64.(Y_data)\n",
    "    if shuffle\n",
    "        JuTools.shuffle_data!(X_data, Y_data)\n",
    "    end\n",
    "    # is it better to use zero weights than normal weights ?\n",
    "    weights = WeightsLinearSVM(C, Random.randn(size(X_data)[2]), Random.randn())\n",
    "    best_cost = nothing\n",
    "    n_cost_no_change = n_iter_no_change\n",
    "    for i in 1:max_iter\n",
    "        if n_cost_no_change <= 0 && early_stop\n",
    "            break\n",
    "        end\n",
    "        learn!(X_data, Y_data, weights, learning_rate)\n",
    "        new_cost = cost(X_data, Y_data, weights)\n",
    "        if verbose\n",
    "            acc = JuTools.compute_accuracy(predict(X_data, weights), Y_data)\n",
    "            println(\"Iter: $i\")\n",
    "            println(\"Cost = $new_cost\")\n",
    "            println(\"Accuracy = $acc\")\n",
    "            println()\n",
    "        end\n",
    "        if early_stop\n",
    "            if best_cost === nothing || isnan(best_cost)\n",
    "                best_cost = new_cost\n",
    "            else\n",
    "                if new_cost > best_cost - tol\n",
    "                    n_cost_no_change -= 1\n",
    "                else\n",
    "                    best_cost = min(new_cost, best_cost)\n",
    "                    n_cost_no_change = n_iter_no_change\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return weights\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1\n",
      "Cost = 17.49437622239612\n",
      "Accuracy = 0.7328571428571429\n",
      "\n",
      "Iter: 2\n",
      "Cost = 15.759469404970652\n",
      "Accuracy = 0.7328571428571429\n",
      "\n",
      "Iter: 3\n",
      "Cost = 14.041864723704366\n",
      "Accuracy = 0.7328571428571429\n",
      "\n",
      "Iter: 4\n",
      "Cost = 12.34138958978907\n",
      "Accuracy = 0.7328571428571429\n",
      "\n",
      "Iter: 5\n",
      "Cost = 10.657873135989934\n",
      "Accuracy = 0.7328571428571429\n",
      "\n",
      "Iter: 6\n",
      "Cost = 8.991146199472801\n",
      "Accuracy = 0.7328571428571429\n",
      "\n",
      "Iter: 7\n",
      "Cost = 7.341041304802785\n",
      "Accuracy = 0.7328571428571429\n",
      "\n",
      "Iter: 8\n",
      "Cost = 5.707392647112458\n",
      "Accuracy = 0.7328571428571429\n",
      "\n",
      "Iter: 9\n",
      "Cost = 4.090036075437951\n",
      "Accuracy = 0.7328571428571429\n",
      "\n",
      "Iter: 10\n",
      "Cost = 2.5478231645134954\n",
      "Accuracy = 0.7757142857142857\n",
      "\n",
      "Iter: 11\n",
      "Cost = 1.4517810681302135\n",
      "Accuracy = 0.8285714285714286\n",
      "\n",
      "Iter: 12\n",
      "Cost = 0.8625868525842104\n",
      "Accuracy = 0.8785714285714286\n",
      "\n",
      "Iter: 13\n",
      "Cost = 0.5845534725127407\n",
      "Accuracy = 0.9071428571428571\n",
      "\n",
      "Iter: 14\n",
      "Cost = 0.49697254099419136\n",
      "Accuracy = 0.9171428571428571\n",
      "\n",
      "Iter: 15\n",
      "Cost = 0.4724237420776445\n",
      "Accuracy = 0.92\n",
      "\n",
      "Iter: 16\n",
      "Cost = 0.4669039302737097\n",
      "Accuracy = 0.9228571428571428\n",
      "\n",
      "Iter: 17\n",
      "Cost = 0.46259178521950856\n",
      "Accuracy = 0.9228571428571428\n",
      "\n",
      "Iter: 18\n",
      "Cost = 0.4583226512675802\n",
      "Accuracy = 0.9228571428571428\n",
      "\n",
      "Iter: 19\n",
      "Cost = 0.4540960993821795\n",
      "Accuracy = 0.9228571428571428\n",
      "\n",
      "Iter: 20\n",
      "Cost = 0.4499117048071928\n",
      "Accuracy = 0.9228571428571428\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WeightsLinearSVM(1.0, [0.4534683639714476, -0.24542848197815037], 0.5737279523597234)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = train_linear(X_train, Y_train, 1.0, learning_rate=0.005, max_iter=20, tol=0.001, verbose=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9233333333333333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JuTools.compute_accuracy(predict(X_test, weights), Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
