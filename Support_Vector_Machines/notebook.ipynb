{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "[Reference 1](https://scikit-learn.org/stable/modules/svm.html)  \n",
    "[Reference 2](https://en.wikipedia.org/wiki/Support_vector_machine)  \n",
    "[Reference 3](http://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM classification problem (directly taken from sklearn website):  \n",
    "\n",
    "Given training vectors $x_i\\in \\mathbb{R}^P$, $i=1,...,n$, and a vector $y\\in\\{1,-1\\}^n$  \n",
    "The goal is to find $w\\in \\mathbb{R}^P$ and $b\\in \\mathbb{R}$ such that prediction given by $\\text{sign}(w^T\\phi(x)+b)$ is accurate for most samples  \n",
    "\n",
    "Solve the following __primal problem__:  \n",
    "$$\\begin{align}\n",
    "\\min_{w,b,\\zeta}\\frac{1}{2}w^Tw+C\\sum^n_{i=1}\\zeta_i\\\\\n",
    "\\text{subject to }y_i(w^T\\phi(x_i)+b)\\geq1-\\zeta_i,\\\\\n",
    "\\zeta_i\\geq0,i=1,...,n\n",
    "\\end{align}$$\n",
    "\n",
    "Intuition is we are maximizing the margin (by minimizing $\\|w\\|^2=w^Tw$), while penalize when a sample is misclassified\n",
    "\n",
    "The __dual problem__ to primal is: \n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_\\alpha\\frac{1}{2}\\alpha^TQ\\alpha-e^T\\alpha\\\\\n",
    "\\text{subject to }y^T\\alpha=0,\\\\\n",
    "0\\leq\\alpha_i\\leq C,i=1,...,n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$e$ is vector of all ones, $Q$ is nxn positive semidefinite matrix, $Q_{ij}\\equiv y_iy_jK(x_i,x_j)$, and $K(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)$ is the kernel  \n",
    "$\\alpha_i$ are dual coefficients, upper-bounded by $C$  \n",
    "Highlighting the fact that training vectors are implicitly mapped to a higher (or infinite) dimensional space by function $\\phi$  \n",
    "\n",
    "After optimization problem solved, the __output__ for a given sample $x$ is:\n",
    "$$\\sum_{i\\in SV}y_i\\alpha_iK(x_i,x)+b$$\n",
    "\n",
    "\n",
    "The problem solved by `liblinear` for `LinearSVC` is a equivant form of primal problem:\n",
    "$$\\min_{w,b}\\frac{1}{2}w^Tw+C\\sum_{i=1}\\max(0,y_i(w^T\\phi(x_i)+b))$$\n",
    "which does not involve inner products between samples, and therefore cannot apply kernel tricks  \n",
    "\n",
    "$$C=\\frac{1}{\\text{alpha}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Random\n",
    "import Statistics\n",
    "import LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../tools.jl\")\n",
    "import .JuTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM Implementation (Hinge Loss)  \n",
    "[Reference](https://stackoverflow.com/questions/48804198/soft-margin-in-linear-support-vector-machine-using-python)  \n",
    "\n",
    "Cost function is:\n",
    "$$J=\\frac{1}{2}w^Tw+\\frac{C}{N}\\sum^N_{i=1}\\max\\Big(0,1-y_i(w^T\\phi(x_i)+b)\\Big)$$\n",
    "\n",
    "Gradient function for $w$ is:\n",
    "$$\\frac{\\partial J}{\\partial w}=w + \\frac{C}{N}\\sum^N_{i=1}\\begin{cases}\n",
    "0 & y_i(w^T\\phi(x_i)+b) \\geq 1\\\\\n",
    "-y_i\\phi(x_i) & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "Gradient function for $b$ is:\n",
    "$$\\frac{\\partial J}{\\partial b}=\\frac{C}{N}\\sum^N_{i=1}\\begin{cases}\n",
    "0 & y_i(w^T\\phi(x_i)+b) \\geq 1\\\\\n",
    "-y_i & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a struct to store weights\n",
    "# this should be returned by a training function\n",
    "# alpha should be treated as constant\n",
    "mutable struct WeightsLinearSVM\n",
    "    C::AbstractFloat\n",
    "    w::Array{T} where T<:AbstractFloat\n",
    "    b::AbstractFloat\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cost (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define cost function for linear SVM\n",
    "# assum Y_data is {-1, 1}\n",
    "function cost(X_data::Array{T} where T<:Number, Y_data::Array{T} where T<:Number, weights::WeightsLinearSVM)::AbstractFloat\n",
    "    @assert ndims(Y_data) == ndims(weights.w) == 1\n",
    "    @assert size(X_data) == (size(Y_data)[1], size(weights.w)[1])\n",
    "    loss_w = 0.5 * (weights.w' * weights.w)\n",
    "    loss_inner = 1.0 .- Y_data .* vec(X_data * weights.w .+ weights.b)\n",
    "    loss_inner .= map(m->max(0.0,m), loss_inner)\n",
    "    loss = loss_w + weights.C * sum(loss_inner) / size(X_data)[1]\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 2)\n",
      "(300, 2)\n",
      "(700,)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "X_data, Y_data = JuTools.data_generate_linear_2d()\n",
    "Y_data .= Y_data .* 2.0 .- 1.0 # convert from {0,1} to {-1,1}\n",
    "X_train, X_test, Y_train, Y_test = JuTools.split_data(X_data, Y_data)\n",
    "println(size(X_train))\n",
    "println(size(X_test))\n",
    "println(size(Y_train))\n",
    "println(size(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3338375609863453"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_test = WeightsLinearSVM(1.0, Random.randn(size(X_data)[2]), Random.randn())\n",
    "cost(X_data, Y_data, weight_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learn! (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the learning function (gradient descent)\n",
    "function learn!(X_data::Array{T} where T<:Number, Y_data::Array{T} where T<:Number, weights::WeightsLinearSVM, alpha::AbstractFloat)\n",
    "    @assert ndims(Y_data) == ndims(weights.w) == 1\n",
    "    @assert size(X_data) == (size(Y_data)[1], size(weights.w)[1])\n",
    "    # compute deciding feature\n",
    "    decide = (Y_data .* (X_data * weights.w .+ weights.b)) .< 1 # (? < 1) will be 1, otherwise 0\n",
    "    # update w\n",
    "    gradient_w = weights.w .+ (weights.C / size(X_data)[1]) .* vec(-(Y_data .* decide)' * X_data)\n",
    "    gradient_w .= gradient_w .* alpha\n",
    "    weights.w .= weights.w .- gradient_w\n",
    "    # update b\n",
    "    gradient_b = (weights.C / size(X_data)[1]) * sum(-(Y_data .* decide))\n",
    "    gradient_b *= alpha\n",
    "    weights.b = weights.b - gradient_b\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define prediction function\n",
    "function predict_proba(X_predict::Array{T} where T<:Number, weights::WeightsLinearSVM)::Array\n",
    "    @assert ndims(X_predict) == 2\n",
    "    @assert size(X_predict)[2] == size(weights.w)[1]\n",
    "    prediction = vec(X_predict * weights.w .+ weights.b)\n",
    "    return prediction\n",
    "end\n",
    "\n",
    "# output prediction is in {-1, 1}\n",
    "function predict(X_predict::Array{T} where T<:Number, weights::WeightsLinearSVM)::Array\n",
    "    @assert ndims(X_predict) == 2\n",
    "    @assert size(X_predict)[2] == size(weights.w)[1]\n",
    "    prediction = vec(X_predict * weights.w .+ weights.b)\n",
    "    prediction .= map(m -> m >= 0 ? 1.0 : -1.0, prediction)\n",
    "    return prediction\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_linear (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training function for linear SVM\n",
    "# assume Y_data is in {-1, 1}\n",
    "# this function is similar to the training function for Logistic Regression (Both are gradient descent)\n",
    "function train_linear(X_data::Array{T} where T<:Number, Y_data::Array{T} where T<:Number, C::AbstractFloat;\n",
    "        learning_rate::AbstractFloat=0.1, max_iter::Integer=1000, n_iter_no_change::Integer=5, tol::AbstractFloat=0.001,\n",
    "        verbose::Bool=false, shuffle::Bool=true, early_stop::Bool=true)::WeightsLinearSVM\n",
    "    @assert ndims(X_data) == ndims(Y_data) + 1 == 2\n",
    "    @assert size(X_data)[1] == size(Y_data)[1]\n",
    "    @assert max_iter >= 0\n",
    "    @assert n_iter_no_change >= 0\n",
    "    @assert tol >= 0\n",
    "    X_data = Float64.(X_data)\n",
    "    Y_data = Float64.(Y_data)\n",
    "    if shuffle\n",
    "        JuTools.shuffle_data!(X_data, Y_data)\n",
    "    end\n",
    "    # is it better to use zero weights than normal weights ?\n",
    "    weights = WeightsLinearSVM(C, Random.randn(size(X_data)[2]), Random.randn())\n",
    "    best_cost = nothing\n",
    "    n_cost_no_change = n_iter_no_change\n",
    "    for i in 1:max_iter\n",
    "        if n_cost_no_change <= 0 && early_stop\n",
    "            break\n",
    "        end\n",
    "        learn!(X_data, Y_data, weights, learning_rate)\n",
    "        new_cost = cost(X_data, Y_data, weights)\n",
    "        if verbose\n",
    "            acc = JuTools.compute_accuracy(predict(X_data, weights), Y_data)\n",
    "            println(\"Iter: $i\")\n",
    "            println(\"Cost = $new_cost\")\n",
    "            println(\"Accuracy = $acc\")\n",
    "            println()\n",
    "        end\n",
    "        if early_stop\n",
    "            if best_cost === nothing || isnan(best_cost)\n",
    "                best_cost = new_cost\n",
    "            else\n",
    "                if new_cost > best_cost - tol\n",
    "                    n_cost_no_change -= 1\n",
    "                else\n",
    "                    best_cost = min(new_cost, best_cost)\n",
    "                    n_cost_no_change = n_iter_no_change\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return weights\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1\n",
      "Cost = 16.657431574743217\n",
      "Accuracy = 0.5414285714285715\n",
      "\n",
      "Iter: 2\n",
      "Cost = 9.413963518935619\n",
      "Accuracy = 0.6457142857142857\n",
      "\n",
      "Iter: 3\n",
      "Cost = 5.33234137345831\n",
      "Accuracy = 0.75\n",
      "\n",
      "Iter: 4\n",
      "Cost = 3.5281280834005777\n",
      "Accuracy = 0.8242857142857143\n",
      "\n",
      "Iter: 5\n",
      "Cost = 2.6568521499330386\n",
      "Accuracy = 0.8628571428571429\n",
      "\n",
      "Iter: 6\n",
      "Cost = 2.186341807893093\n",
      "Accuracy = 0.8871428571428571\n",
      "\n",
      "Iter: 7\n",
      "Cost = 1.90797554696231\n",
      "Accuracy = 0.9042857142857142\n",
      "\n",
      "Iter: 8\n",
      "Cost = 1.722182047908384\n",
      "Accuracy = 0.9142857142857143\n",
      "\n",
      "Iter: 9\n",
      "Cost = 1.6071265113495203\n",
      "Accuracy = 0.9228571428571428\n",
      "\n",
      "Iter: 10\n",
      "Cost = 1.5186258246554019\n",
      "Accuracy = 0.9328571428571428\n",
      "\n",
      "Iter: 11\n",
      "Cost = 1.4651134920304805\n",
      "Accuracy = 0.9385714285714286\n",
      "\n",
      "Iter: 12\n",
      "Cost = 1.4291375642963189\n",
      "Accuracy = 0.94\n",
      "\n",
      "Iter: 13\n",
      "Cost = 1.4007213955530369\n",
      "Accuracy = 0.9371428571428572\n",
      "\n",
      "Iter: 14\n",
      "Cost = 1.3780564679122271\n",
      "Accuracy = 0.9357142857142857\n",
      "\n",
      "Iter: 15\n",
      "Cost = 1.3565240720727472\n",
      "Accuracy = 0.9371428571428572\n",
      "\n",
      "Iter: 16\n",
      "Cost = 1.3360334784472339\n",
      "Accuracy = 0.94\n",
      "\n",
      "Iter: 17\n",
      "Cost = 1.3171967312474888\n",
      "Accuracy = 0.94\n",
      "\n",
      "Iter: 18\n",
      "Cost = 1.2997238432281697\n",
      "Accuracy = 0.9414285714285714\n",
      "\n",
      "Iter: 19\n",
      "Cost = 1.284731236955754\n",
      "Accuracy = 0.9414285714285714\n",
      "\n",
      "Iter: 20\n",
      "Cost = 1.2702340164103252\n",
      "Accuracy = 0.9414285714285714\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WeightsLinearSVM(1.0, [1.1434069442314625, -0.5798482024728584], 0.8271665511250503)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = train_linear(X_train, Y_train, 1.0, learning_rate=0.005, max_iter=20, tol=0.001, verbose=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9033333333333333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JuTools.compute_accuracy(predict(X_test, weights), Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM (with various kernels)  \n",
    "[Reference](https://en.wikipedia.org/wiki/Quadratic_programming)  \n",
    "\n",
    "For solving SVM minimization problem:  \n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_\\alpha\\frac{1}{2}\\alpha^TQ\\alpha-e^T\\alpha\\\\\n",
    "\\text{subject to }y^T\\alpha=0,\\\\\n",
    "0\\leq\\alpha_i\\leq C,i=1,...,n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $e$ is vector of all ones, $Q$ is nxn positive semidefinite matrix, $Q_{ij}\\equiv y_iy_jK(x_i,x_j)$, and $K(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)$ is the kernel  \n",
    "$\\alpha_i$ are dual coefficients, upper-bounded by $C$  \n",
    "\n",
    "We'll be using a Quatratic Programming technique: [Sequential Minimal Optimization](https://en.wikipedia.org/wiki/Sequential_minimal_optimization)(SMO)  \n",
    "\n",
    "My implementation is inspired from [this blog](https://jonchar.net/notebooks/SVM/), whose code is originally from [this paper](https://www.researchgate.net/publication/234786663_Fast_Training_of_Support_Vector_Machines_Using_Sequential_Minimal_Optimization)  \n",
    "\n",
    "The problem can also be written into (objective function):\n",
    "$$\n",
    "\\begin{align}\n",
    "\\max_\\alpha e^T\\alpha-\\frac{1}{2}\\alpha^TQ\\alpha\\\\\n",
    "\\text{subject to }y^T\\alpha=0,\\\\\n",
    "0\\leq\\alpha_i\\leq C,i=1,...,n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The kernel functions that I'm going to implement are (using sklearn names):  \n",
    "* Linear Kernel: $\\langle x_i,x_j \\rangle$  \n",
    "* Polynomial Kernel: $(\\gamma\\langle x_i,x_j \\rangle + r)^d$ ($d$ is degree, and $r$ is coeficient, $\\gamma$ is a parameter)  \n",
    "* Rbf Kernel: $\\exp(-\\gamma\\|x_i-x_j\\|^2)$, where $\\gamma$ is a parameter  \n",
    "* Sigmoid Kernel: $\\tanh(\\gamma\\langle x_i,x_j \\rangle + r)$, where $r$ and $\\gamma$ are parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a struct to store information\n",
    "mutable struct WeightsSVM\n",
    "    C::AbstractFloat                # constraint\n",
    "    b::AbstractFloat                # threshold\n",
    "    gamma::AbstractFloat            # parameter used for polynomial, rbf, and sigmoid kernels\n",
    "    r::AbstractFloat                # parameter used for polynomial, and sigmoid kernels\n",
    "    d::AbstractFloat                # parameter used for polynomial kernel\n",
    "    tol_alpha::AbstractFloat        # tolerance for alpha\n",
    "    tol_error::AbstractFloat        # tolerance for error\n",
    "    alpha::Array{T} where T<:Number # alpha array\n",
    "    error::Array{T} where T<:Number # array for error cache\n",
    "    kernel::String                  # kernel function name\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kernel_sigmoid (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear kernel\n",
    "function kernel_linear(X1::Array{T} where T<:Number, X2::Array{T} where T<:Number)::Array\n",
    "    @assert ndims(X1) == ndims(X2) == 2\n",
    "    @assert size(X1)[2] == size(X2)[2]\n",
    "    result = X1 * X2'\n",
    "    return result\n",
    "end\n",
    "# polynomial kernel\n",
    "function kernel_polynomial(X1::Array{T} where T<:Number, X2::Array{T} where T<:Number;\n",
    "        d::AbstractFloat=1.0, r::AbstractFloat=0.0, gamma::AbstractFloat=1.0)::Array\n",
    "    @assert ndims(X1) == ndims(X2) == 2\n",
    "    @assert size(X1)[2] == size(X2)[2]\n",
    "    result = (gamma .* (X1 * X2') .+ r) .^ d\n",
    "    return result\n",
    "end\n",
    "# rbf kernel\n",
    "function kernel_rbf(X1::Array{T} where T<:Number, X2::Array{T} where T<:Number; gamma::AbstractFloat=1.0)::Array\n",
    "    @assert ndims(X1) == ndims(X2) == 2\n",
    "    @assert size(X1)[2] == size(X2)[2]\n",
    "    result = (sum(X1 .^ 2, dims=2) * ones(size(X2)[1])') .+ (ones(size(X1)[1]) * sum(X2 .^ 2, dims=2)') .- 2.0 .* (X1 * X2')\n",
    "    result .= broadcast(m->max(0.0, m), result) # ignore very small negative outputs, due to precision\n",
    "    result .= sqrt.(result)\n",
    "    result .= (-gamma) .* result\n",
    "    result .= exp.(result)\n",
    "    return result\n",
    "end\n",
    "# sigmoid kernel\n",
    "function kernel_sigmoid(X1::Array{T} where T<:Number, X2::Array{T} where T<:Number; gamma::AbstractFloat=1.0, r::AbstractFloat=0.0)::Array\n",
    "    @assert ndims(X1) == ndims(X2) == 2\n",
    "    @assert size(X1)[2] == size(X2)[2]\n",
    "    result = gamma .* (X1 * X2') .+ r\n",
    "    result .= tanh.(gamma)\n",
    "    return result\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cost (generic function with 2 methods)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# because we call it cost function, we will use the original formula\n",
    "function cost(X_data::Array{T} where T<:Number, weights::WeightsSVM)::AbstractFloat\n",
    "    @assert ndims(X_data) == ndims(weights.alpha) + 1 == 2\n",
    "    @assert size(X_data)[1] == size(weights.alpha)[1]\n",
    "    result = nothing\n",
    "    if weights.kernel == \"linear\"\n",
    "        result = kernel_linear(X_data, X_data)\n",
    "    elseif weights.kernel == \"polynomial\"\n",
    "        result = kernel_polynomial(X_data, X_data, d=weights.d, r=weights.r, gamma=weights.gamma)\n",
    "    elseif weights.kernel == \"rbf\"\n",
    "        result = kernel_rbf(X_data, X_data, gamma=weights.gamma)\n",
    "    elseif weights.kernel == \"sigmoid\"\n",
    "        result = kernel_sigmoid(X_data, X_data, gamma=weights.gamma, r=weights.r)\n",
    "    else\n",
    "        throw(ArgumentError(\"Error: kernel function $weights.kernel is not recognized\"))\n",
    "    end\n",
    "    result = 0.5 * (weights.alpha' * result * weights.alpha) - sum(weights.alpha)\n",
    "    return result\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learn_step! (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define learning each step function\n",
    "# update weights in place, and return num steps\n",
    "function learn_step!(X_data::Array{T} where T<:Number, Y_data::Array{T} where T<:Number,\n",
    "        weights::WeightsSVM, id1::Integer, id2::Integer)::Integer\n",
    "    @assert ndims(X_data) == ndims(Y_data) + 1 == 2\n",
    "    @assert size(X_data)[1] == size(Y_data)[1]\n",
    "    @assert size(X_data)[1] == size(weights.alpha)[1]\n",
    "    @assert size(weights.error) == size(weights.alpha)\n",
    "    @assert id1 >= 1\n",
    "    @assert id2 >= 1\n",
    "    # if choosing same alpha, skip\n",
    "    if id1 == id2\n",
    "        return 0\n",
    "    end\n",
    "    # prepare data\n",
    "    alpha1 = weights.alpha[id1]\n",
    "    alpha2 = weights.alpha[id2]\n",
    "    Y1 = Y_data[id1]\n",
    "    Y2 = Y_data[id2]\n",
    "    error1 = weights.error[id1]\n",
    "    error2 = weights.error[id2]\n",
    "    # compute L & H\n",
    "    L = nothing\n",
    "    H = nothing\n",
    "    if Y1 != Y2\n",
    "        L = max(0.0, alpha2 - alpha1)\n",
    "        H = min(weights.C, weights.C + alpha2 - alpha1)\n",
    "    else\n",
    "        L = max(0.0, alpha1 + alpha2 - weights.C)\n",
    "        H = min(weights.C, alpha1 + alpha2)\n",
    "    end\n",
    "    if L == H\n",
    "        return 0\n",
    "    end\n",
    "    # compute kernel results and 2nd derivative eta\n",
    "    k11 = nothing\n",
    "    k12 = nothing\n",
    "    k22 = nothing\n",
    "    n_features = size(X_data)[2]\n",
    "    X_id1 = reshape(X_data[id1, :], (1, n_features))\n",
    "    X_id2 = reshape(X_data[id2, :], (1, n_features))\n",
    "    if weights.kernel == \"linear\"\n",
    "        k11 = kernel_linear(X_id1, X_id1)[1]\n",
    "        k12 = kernel_linear(X_id1, X_id2)[1]\n",
    "        k22 = kernel_linear(X_id2, X_id2)[1]\n",
    "    elseif weights.kernel == \"polynomial\"\n",
    "        k11 = kernel_polynomial(X_id1, X_id1, d=weights.d, r=weights.r, gamma=weights.gamma)[1]\n",
    "        k12 = kernel_polynomial(X_id1, X_id2, d=weights.d, r=weights.r, gamma=weights.gamma)[1]\n",
    "        k22 = kernel_polynomial(X_id2, X_id2, d=weights.d, r=weights.r, gamma=weights.gamma)[1]\n",
    "    elseif weights.kernel == \"rbf\"\n",
    "        k11 = kernel_rbf(X_id1, X_id1, gamma=weights.gamma)[1]\n",
    "        k12 = kernel_rbf(X_id1, X_id2, gamma=weights.gamma)[1]\n",
    "        k22 = kernel_rbf(X_id2, X_id2, gamma=weights.gamma)[1]\n",
    "    elseif weights.kernel == \"sigmoid\"\n",
    "        k11 = kernel_sigmoid(X_id1, X_id1, gamma=weights.gamma, r=weights.r)[1]\n",
    "        k12 = kernel_sigmoid(X_id1, X_id2, gamma=weights.gamma, r=weights.r)[1]\n",
    "        k22 = kernel_sigmoid(X_id2, X_id2, gamma=weights.gamma, r=weights.r)[1]\n",
    "    else\n",
    "        throw(ArgumentError(\"Error: kernel function $weights.kernel is not recognized\"))\n",
    "    end\n",
    "    eta = 2 * k12 - k11 - k22\n",
    "    # compute new alpha2 (a2)\n",
    "    a2 = nothing\n",
    "    if eta < 0.0\n",
    "        a2 = alpha2 - Y2 * (error1 - error2) / eta\n",
    "        a2 = min(a2, H)\n",
    "        a2 = max(a2, L)\n",
    "    else\n",
    "        weights.alpha[id2] = L\n",
    "        Lobj = -cost(X_data, weights)\n",
    "        weights.alpha[id2] = H\n",
    "        Hobj = -cost(X_data, weights)\n",
    "        weights.alpha[id2] = alpha2\n",
    "        if Lobj > (Hobj + weights.tol_alpha)\n",
    "            a2 = L\n",
    "        elseif Lobj < (Hobj - weights.tol_alpha)\n",
    "            a2 = H\n",
    "        else\n",
    "            a2 = alpha2\n",
    "        end\n",
    "    end\n",
    "    # push to 0 or C\n",
    "    if a2 < 1e-8\n",
    "        a2 = 0.0\n",
    "    elseif a2 > (weights.C - 1e-8)\n",
    "        a2 = weights.C\n",
    "    end\n",
    "    # skip if cannot be optimized\n",
    "    if abs(a2 - alpha2) < weights.tol_alpha * (a2 + alpha2 + weights.tol_alpha)\n",
    "        return 0\n",
    "    end\n",
    "    # compute new alpha1 (a1)\n",
    "    a1 = alpha1 + (Y1 * Y2) * (alpha2 - a2)\n",
    "    if a1 < 0.0\n",
    "        a2 += (Y1 * Y2) * a1\n",
    "        a1 = 0.0\n",
    "    elseif a1 > weights.C\n",
    "        a2 += (Y1 * Y2) * (a1 - weights.C)\n",
    "        a1 = weights.C\n",
    "    end\n",
    "    # update threshold\n",
    "    b1 = error1 + Y1 * (a1 - alpha1) * k11 + Y2 * (a2 - alpha2) * k12 + weights.b\n",
    "    b2 = error2 + Y1 * (a1 - alpha1) * k12 + Y2 * (a2 - alpha2) * k22 + weights.b\n",
    "    b_new = nothing\n",
    "    if 0 < a1 < weights.C\n",
    "        b_new = b1\n",
    "    elseif 0 < a2 < weights.C\n",
    "        b_new = b2\n",
    "    else\n",
    "        b_new = (b1 + b2) * 0.5\n",
    "    end\n",
    "    # update error cache\n",
    "    non_optimized_ids = [i for i in 1:size(X_data)[1] if (i != id1 && i != id2 && (0 < weights.alpha[i] < weights.C))]\n",
    "    kerr1 = nothing\n",
    "    kerr2 = nothing\n",
    "    if weights.kernel == \"linear\"\n",
    "        kerr1 = vec(kernel_linear(X_id1, X_data[non_optimized_ids, :]))\n",
    "        kerr2 = vec(kernel_linear(X_id2, X_data[non_optimized_ids, :]))\n",
    "    elseif weights.kernel == \"polynomial\"\n",
    "        kerr1 = vec(kernel_polynomial(X_id1, X_data[non_optimized_ids, :], d=weights.d, r=weights.r, gamma=weights.gamma))\n",
    "        kerr2 = vec(kernel_polynomial(X_id2, X_data[non_optimized_ids, :], d=weights.d, r=weights.r, gamma=weights.gamma))\n",
    "    elseif weights.kernel == \"rbf\"\n",
    "        kerr1 = vec(kernel_rbf(X_id1, X_data[non_optimized_ids, :], gamma=weights.gamma))\n",
    "        kerr2 = vec(kernel_rbf(X_id2, X_data[non_optimized_ids, :], gamma=weights.gamma))\n",
    "    elseif weights.kernel == \"sigmoid\"\n",
    "        kerr1 = vec(kernel_sigmoid(X_id1, X_data[non_optimized_ids, :], gamma=weights.gamma, r=weights.r))\n",
    "        kerr2 = vec(kernel_sigmoid(X_id2, X_data[non_optimized_ids, :], gamma=weights.gamma, r=weights.r))\n",
    "    end\n",
    "    weights.error[non_optimized_ids] .= weights.error[non_optimized_ids] .+\n",
    "        ((Y1*(a1-alpha1)) .* kerr1) .+ ((Y2*(a2-alpha2)) .* kerr2) .+ (b_new - weights.b)\n",
    "    weights.error[id1] = 0.0\n",
    "    weights.error[id2] = 0.0\n",
    "    # update alpha and b\n",
    "    weights.b = b_new\n",
    "    weights.alpha[id1] = a1\n",
    "    weights.alpha[id2] = a2\n",
    "    return 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learn! (generic function with 2 methods)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now define the learning function\n",
    "function learn!(X_data::Array{T} where T<:Number, Y_data::Array{T} where T<:Number,\n",
    "        weights::WeightsSVM, id::Integer; verbose::Bool=false)::Integer\n",
    "    @assert ndims(X_data) == ndims(Y_data) + 1 == 2\n",
    "    @assert size(X_data)[1] == size(Y_data)[1]\n",
    "    @assert size(X_data)[1] == size(weights.alpha)[1]\n",
    "    @assert size(weights.error) == size(weights.alpha)\n",
    "    @assert id >= 1\n",
    "    Y = Y_data[id]\n",
    "    alpha = weights.alpha[id]\n",
    "    error = weights.error[id]\n",
    "    r = error * Y\n",
    "    if ((r < -weights.tol_error) && (alpha < weights.C)) || ((r > weights.tol_error) && (alpha > 0))\n",
    "        alpha_target = [i for (i, m) in enumerate(weights.alpha) if (0.0 < m < weights.C)]\n",
    "        # try argmax E1 - E2\n",
    "        new_id = -1\n",
    "        tmax = 0\n",
    "        if verbose\n",
    "            println(\"Trying argmax(abs(E1 - E2))\")\n",
    "        end\n",
    "        for i in alpha_target\n",
    "            tmp = abs(error - weights.error[i])\n",
    "            if(tmp > tmax)\n",
    "                tmax = tmp\n",
    "                new_id = i\n",
    "            end\n",
    "        end\n",
    "        if new_id >= 1\n",
    "            step = learn_step!(X_data, Y_data, weights, id, new_id)\n",
    "            if step > 0\n",
    "                return step\n",
    "            end\n",
    "        end\n",
    "        # loop non-bound alphas, randomly\n",
    "        if verbose\n",
    "            println(\"Trying random non-bound alphas\")\n",
    "        end\n",
    "        for new_id in alpha_target[Random.randperm(length(alpha_target))]\n",
    "            step = learn_step!(X_data, Y_data, weights, id, new_id)\n",
    "            if step > 0\n",
    "                return step\n",
    "            end\n",
    "        end\n",
    "        # else loop all alphas, randomly\n",
    "        if verbose\n",
    "            println(\"Trying random remaining alphas\")\n",
    "        end\n",
    "        for new_id in Random.randperm(length(weights.alpha))\n",
    "            if new_id in alpha_target\n",
    "                continue # skip the alpha ids that already looked at\n",
    "            end\n",
    "            step = learn_step!(X_data, Y_data, weights, id, new_id)\n",
    "            if step > 0\n",
    "                return step\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return 0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 2 methods)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implement predict functions\n",
    "function predict_proba(X_predict::Array{T} where T<:Number, X_data::Array{T} where T<:Number,\n",
    "        Y_data::Array{T} where T<:Number, weights::WeightsSVM)::Array\n",
    "    @assert ndims(X_predict) == ndims(X_data) == ndims(Y_data) + 1 == 2\n",
    "    @assert size(X_predict)[2] == size(X_data)[2]\n",
    "    result = nothing\n",
    "    if weights.kernel == \"linear\"\n",
    "        result = kernel_linear(X_data, X_predict)\n",
    "    elseif weights.kernel == \"polynomial\"\n",
    "        result = kernel_polynomial(X_data, X_predict, d=weights.d, r=weights.r, gamma=weights.gamma)\n",
    "    elseif weights.kernel == \"rbf\"\n",
    "        result = kernel_rbf(X_data, X_predict, gamma=weights.gamma)\n",
    "    elseif weights.kernel == \"sigmoid\"\n",
    "        result = kernel_sigmoid(X_data, X_predict, gamma=weights.gamma, r=weights.r)\n",
    "    else\n",
    "        throw(ArgumentError(\"Error: kernel function $weights.kernel is not recognized\"))\n",
    "    end\n",
    "    prediction = vec((weights.alpha .* Y_data)' * result)\n",
    "    return prediction\n",
    "end\n",
    "\n",
    "function predict(X_predict::Array{T} where T<:Number, X_data::Array{T} where T<:Number,\n",
    "        Y_data::Array{T} where T<:Number, weights::WeightsSVM)::Array\n",
    "    @assert ndims(X_predict) == ndims(X_data) == ndims(Y_data) + 1 == 2\n",
    "    @assert size(X_predict)[2] == size(X_data)[2]\n",
    "    result = nothing\n",
    "    if weights.kernel == \"linear\"\n",
    "        result = kernel_linear(X_data, X_predict)\n",
    "    elseif weights.kernel == \"polynomial\"\n",
    "        result = kernel_polynomial(X_data, X_predict, d=weights.d, r=weights.r, gamma=weights.gamma)\n",
    "    elseif weights.kernel == \"rbf\"\n",
    "        result = kernel_rbf(X_data, X_predict, gamma=weights.gamma)\n",
    "    elseif weights.kernel == \"sigmoid\"\n",
    "        result = kernel_sigmoid(X_data, X_predict, gamma=weights.gamma, r=weights.r)\n",
    "    else\n",
    "        throw(ArgumentError(\"Error: kernel function $weights.kernel is not recognized\"))\n",
    "    end\n",
    "    prediction = vec((weights.alpha .* Y_data)' * result)\n",
    "    prediction .= map(m -> m >= 0 ? 1.0 : -1.0, prediction)\n",
    "    return prediction\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally implement the training function\n",
    "function train(X_data::Array{T} where T<:Number, Y_data::Array{T} where T<:Number, C::AbstractFloat;\n",
    "        tol_alpha::AbstractFloat=0.001, tol_error::AbstractFloat=0.001, kernel::String=\"rbf\", gamma::String=\"scale\",\n",
    "        degree::AbstractFloat=1.0, coef::AbstractFloat=0.0, verbose::Bool=false)::WeightsSVM\n",
    "    @assert ndims(X_data) == ndims(Y_data) + 1 == 2\n",
    "    @assert size(X_data)[1] == size(Y_data)[1]\n",
    "    X_data = Float64.(X_data)\n",
    "    Y_data = Float64.(Y_data)\n",
    "    # gamma is computed the same way sklearn does\n",
    "    gamma_num = nothing\n",
    "    if gamma == \"scale\"\n",
    "        gamma_num = 1.0 / (size(X_data)[2] * Statistics.var(X_data))\n",
    "    elseif gamma == \"auto\"\n",
    "        gamma_num = 1.0 / size(X_data)[2]\n",
    "    else\n",
    "        throw(ArgumentError(\"Error: gamma $gamma is not recognized, possible values are 'scale' and 'auto'\"))\n",
    "    end\n",
    "    weights = WeightsSVM(C, 0.0, gamma_num, coef, degree, tol_alpha, tol_error, Float64.(zeros(size(X_data)[1])), -copy(Y_data), kernel)\n",
    "    num_changed = 0\n",
    "    examine_all = true\n",
    "    total_steps = 0\n",
    "    while (num_changed > 0) || examine_all\n",
    "        num_changed = 0\n",
    "        if examine_all\n",
    "            if verbose\n",
    "                println(\"Scanning all training data\")\n",
    "            end\n",
    "            for i in 1:size(X_data)[1]\n",
    "                step = learn!(X_data, Y_data, weights, i, verbose=verbose)\n",
    "                num_changed += step\n",
    "                if step > 0 && verbose\n",
    "                    obj = -cost(X_data, weights)\n",
    "                    println(\"1 step further, objective = $obj\")\n",
    "                end\n",
    "            end\n",
    "        else\n",
    "            if verbose\n",
    "                println(\"Scanning data whose alpha is not at limit\")\n",
    "            end\n",
    "            alpha_target = [i for (i, m) in enumerate(weights.alpha) if (m != 0.0 && m != weights.C)]\n",
    "            for i in alpha_target\n",
    "                step = learn!(X_data, Y_data, weights, i, verbose=verbose)\n",
    "                num_changed += step\n",
    "                if step > 0 && verbose\n",
    "                    obj = -cost(X_data, weights)\n",
    "                    println(\"1 step further, objective = $obj\")\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        if examine_all\n",
    "            examine_all = false\n",
    "        elseif num_changed <= 0\n",
    "            examine_all = true\n",
    "        end\n",
    "        total_steps += num_changed\n",
    "    end\n",
    "    if verbose\n",
    "        println(\"Training Complete\\nTotal steps: $total_steps\")\n",
    "    end\n",
    "    return weights\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeightsSVM(10.0, -9.960147874127775e32, 0.5, 0.0, 2.0, 0.001, 0.001, [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 10.0, 0.0, 10.0, 0.0, 1.7763568394002505e-15, 10.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \"rbf\")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = train(X_train, Y_train, 10.0, kernel=\"rbf\", verbose=false, gamma=\"auto\", coef=0.0, degree=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8566666666666667\n"
     ]
    }
   ],
   "source": [
    "println(JuTools.compute_accuracy(predict(X_test, X_train, Y_train, weights), Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.001421 seconds (21 allocations: 3.745 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.00173568885e10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.kernel = \"linear\"\n",
    "@time cost(X_train, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.010023 seconds (38 allocations: 18.742 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15405.078765266102"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.kernel = \"rbf\"\n",
    "@time cost(X_train, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.069244 seconds (274.95 k allocations: 21.214 MiB, 12.86% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.569316899107224e13"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.kernel = \"polynomial\"\n",
    "@time cost(X_train, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.080557 seconds (304.81 k allocations: 22.617 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0758669044561507e6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.kernel = \"sigmoid\"\n",
    "@time cost(X_train, weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
