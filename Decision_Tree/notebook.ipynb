{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree  \n",
    "\n",
    "[Reference 1](https://en.wikipedia.org/wiki/Decision_tree)  \n",
    "[Reference 2](https://scikit-learn.org/stable/modules/tree.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4.5 Approach\n",
    "\n",
    "Algorithm described [here](https://en.wikipedia.org/wiki/C4.5_algorithm)  \n",
    "My implementation is inspired by the description on [this page](https://cis.temple.edu/~giorgio/cis587/readings/id3-c45.html)\n",
    "\n",
    "Use __Entropy__ to determine purity:  \n",
    "$$\\text{Entropy}(S)=\\sum^c_{i=1}-p_i\\log_2(p_i)$$  \n",
    "For a given segment of data $S$, $c$ is the number of different class levels, and $p_i$ is the proportion of values falling into class level $i$  \n",
    "Entropy of 0 means that data is homogenous, and 1 indicates the maximum amount of disorder  \n",
    "\n",
    "Then compute __information gain__ for each feature to decide which to split on:  \n",
    "$$\\text{InfoGain}(F)=\\text{Entropy}(S_1)-\\text{Entropy}(S_2)$$  \n",
    "where $F$ is a feature, $S_1$ is the segment before split, and $S_2$ is the partitions after split  \n",
    "Entropy on $S_2$ is the weighed sum of entropy on all its partitions ($P_i$), $w_i$ is the proportion of samples falled into $P_i$  \n",
    "$$\\text{Entropy}(S_2)=\\sum^n_{i=1}w_i\\text{Entropy}(P_i)$$  \n",
    "The larger the information gain, the better a feature is splitted to be homogenous  \n",
    "\n",
    "An improvement on information gain is __gain ratios__:  \n",
    "$$\\text{GainRatio}=\\frac{\\text{InfoGain}(F)}{\\text{SplitInfo}(F)}$$  \n",
    "where $\\text{SplitInfo}$ is the entropy due to the split of $F$  \n",
    "$$\\text{SplitInfo}=\\sum^c_{j=1}-P_j\\log_2(P_j)$$  \n",
    "where each $P_j$ is the proportion of values after partitioning $F$, without considering target classes like entropy function\n",
    "\n",
    "__Pre-pruning__ (early stopping): As the tree will grow large by dividing features and overfit, we can stop the growing tree after it reaches certain amount of decisions  \n",
    "\n",
    "__Post-pruning__: After the tree grows to its maximum size, cut out nodes and branches that have little effect on accuracy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../tools.jl\")\n",
    "import .JuTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "X_data, Y_data = JuTools.data_generate_cluster_2d(pos1=(30.0, 80.0), pos2=(80.0, 30.0),\n",
    "    radius1=5.0, radius2=10.0, random_scale=8.0, data_size=1000)\n",
    "println(size(X_data))\n",
    "println(size(Y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct DecisionTree\n",
    "    col_id::Integer\n",
    "    children::Union{AbstractDict{Function,DecisionTree},Nothing}\n",
    "    target::Union{AbstractFloat,Nothing}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_decision_tree (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function create_decision_tree(X_data::Array{T} where T<:Number, Y_data::Array{T} where T<:Number; max_depth::Integer=10)::DecisionTree\n",
    "    @assert ndims(X_data) == ndims(Y_data) + 1 == 2\n",
    "    @assert size(X_data)[1] == size(Y_data)[1]\n",
    "    @assert max_depth >= 1\n",
    "    \n",
    "    function majority(Y_vec::Array)::Number\n",
    "        unique_vals = Dict{Number, Integer}()\n",
    "        for Y_val in Y_vec\n",
    "            if !haskey(unique_vals, Y_val)\n",
    "                unique_vals[Y_val] = 1\n",
    "            else\n",
    "                unique_vals[Y_val] += 1\n",
    "            end\n",
    "        end\n",
    "        result = sort(collect(unique_vals), by=m->m[2])\n",
    "        return result[end][1]\n",
    "    end\n",
    "    \n",
    "    function is_same_class(Y_vec::Array)::Bool\n",
    "        val = Y_vec[1]\n",
    "        for m in Y_vec[2:end]\n",
    "            if m != val\n",
    "                return false\n",
    "            end\n",
    "        end\n",
    "        return true\n",
    "    end\n",
    "    \n",
    "    function get_unique_array(X_vec::Array)::Array\n",
    "        X_max = maximum(X_vec)\n",
    "        X_min = minimum(X_vec)\n",
    "        round_digits = -Integer(trunc(log10(X_max-X_min)-2))\n",
    "        result = []\n",
    "        for m in X_vec\n",
    "            if !(trunc(m, digits=round_digits) in result)\n",
    "                push!(result, m)\n",
    "            end\n",
    "        end\n",
    "        return result\n",
    "    end\n",
    "    \n",
    "    function info(Y_data::Array)::AbstractFloat\n",
    "        portion = Dict{Number,Integer}()\n",
    "        for m in Y_data\n",
    "            if !haskey(portion, m)\n",
    "                portion[m] = 1\n",
    "            else\n",
    "                portion[m] += 1\n",
    "            end\n",
    "        end\n",
    "        T = size(Y_data)[1]\n",
    "        result = 0.0\n",
    "        for m in keys(portion)\n",
    "            p = portion[m] / T\n",
    "            result += -p * log2(p)\n",
    "        end\n",
    "        return result\n",
    "    end\n",
    "    \n",
    "    function create_decision_tree_recursive(X_data::Array, Y_data::Array, X_index_visited::Array, max_depth::Integer)::DecisionTree\n",
    "        if is_same_class(Y_data)\n",
    "            return DecisionTree(-1, nothing, Y_data[1])\n",
    "        end\n",
    "        if max_depth <= 0\n",
    "            return DecisionTree(-1, nothing, majority(Y_data))\n",
    "        end\n",
    "        max_index = -1\n",
    "        max_ratio = 0.0\n",
    "        max_fn = nothing\n",
    "        for i in 1:size(X_data)[2]\n",
    "            if i in X_index_visited\n",
    "                continue\n",
    "            end\n",
    "            X_vec_unique = get_unique_array(X_data[:, i])\n",
    "            if length(X_vec_unique) > 10\n",
    "                # continous data, then choose binary threshold\n",
    "                fn_max = nothing\n",
    "                ratio_max = 0\n",
    "                for j in 1:(length(X_vec_unique)-1)\n",
    "                    # choose average as threshold\n",
    "                    threshold = (X_vec_unique[j] + X_vec_unique[j+1]) / 2.0\n",
    "                    fn = (m) -> m <= threshold\n",
    "                    # compute information gain\n",
    "                    Ii = info(Y_data)\n",
    "                    Y_1 = Y_data[fn.(X_data[:, i])]\n",
    "                    Y_2 = Y_data[(!fn).(X_data[:, i])]\n",
    "                    I1 = info(Y_1)\n",
    "                    I2 = info(Y_2)\n",
    "                    gain = Ii - (length(Y_1)/length(Y_data)*I1 + length(Y_2)/length(Y_data)*I2)\n",
    "                    splitInfo = -(length(Y_1)/length(Y_data))*log2(length(Y_1)/length(Y_data)) - (length(Y_2)/length(Y_data))*log2(length(Y_2)/length(Y_data))\n",
    "                    ratio = gain / splitInfo\n",
    "                    if fn_max === nothing\n",
    "                        fn_max = fn\n",
    "                        ratio_max = ratio\n",
    "                    else\n",
    "                        if ratio > ratio_max\n",
    "                            fn_max = fn\n",
    "                            ratio_max = ratio\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "                if ratio_max > max_ratio\n",
    "                    max_index = i\n",
    "                    max_fn = fn_max\n",
    "                    max_ratio = ratio_max\n",
    "                end\n",
    "            else\n",
    "                # categorical data, then compute by splitting on each category\n",
    "                Ii = info(Y_data)\n",
    "                gain = Ii\n",
    "                splitInfo = 0.0\n",
    "                for val in X_vec_unique\n",
    "                    fn = (m) -> m == val\n",
    "                    Y0 = Y_data[fn.(X_data[:, i])]\n",
    "                    I0 = info(Y0)\n",
    "                    gain -= (length(Y0)/length(Y_data))*I0\n",
    "                    splitInfo -= (length(Y0)/length(Y_data))*log2(length(Y0)/length(Y_data))\n",
    "                end\n",
    "                ratio = gain / splitInfo\n",
    "                if ratio > max_ratio\n",
    "                    max_index = i\n",
    "                    max_fn = nothing\n",
    "                    max_ratio = ratio\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        if max_index < 0\n",
    "            return DecisionTree(-1, nothing, majority(Y_data))\n",
    "        end\n",
    "        X_index_visited = copy(X_index_visited)\n",
    "        push!(X_index_visited, max_index)\n",
    "        children = Dict{Function,DecisionTree}()\n",
    "        if max_fn === nothing\n",
    "            # categorical\n",
    "            X_vec_unique = get_unique_array(X_data[:, max_index])\n",
    "            for val in X_vec_unique\n",
    "                fn = (m) -> m == val\n",
    "                identify = fn.(X_data[:, max_index])\n",
    "                X_part = X_data[identify, :]\n",
    "                Y_part = Y_data[identity]\n",
    "                children[fn] = create_decision_tree_recursive(X_part, Y_part, X_index_visited, max_depth-1)\n",
    "            end\n",
    "        else\n",
    "            # continuous\n",
    "            identify_1 = max_fn.(X_data[:, max_index])\n",
    "            X_part_1 = X_data[identify_1, :]\n",
    "            Y_part_1 = Y_data[identify_1]\n",
    "            children[max_fn] = create_decision_tree_recursive(X_part_1, Y_part_1, X_index_visited, max_depth-1)\n",
    "            identify_2 = (!max_fn).(X_data[:, max_index])\n",
    "            X_part_2 = X_data[identify_2, :]\n",
    "            Y_part_2 = Y_data[identify_2]\n",
    "            children[(!max_fn)] = create_decision_tree_recursive(X_part_2, Y_part_2, X_index_visited, max_depth-1)\n",
    "        end\n",
    "        return DecisionTree(max_index, children, nothing)\n",
    "    end\n",
    "    \n",
    "    return create_decision_tree_recursive(X_data, Y_data, [], max_depth)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(X_data::Array{T} where T<:Number, tree::DecisionTree)::Array\n",
    "    if ndims(X_data) == 1\n",
    "        X_data = reshape(X_data, (1, size(X_data)[1]))\n",
    "    end\n",
    "    @assert ndims(X_data) == 2\n",
    "    prediction = []\n",
    "    for i in 1:size(X_data)[1]\n",
    "        X_vec = X_data[i, :]\n",
    "        Y_pred = nothing\n",
    "        tree_copy = tree\n",
    "        while true\n",
    "            if tree_copy.target !== nothing\n",
    "                Y_pred = tree_copy.target\n",
    "                break\n",
    "            end\n",
    "            @assert tree_copy.children !== nothing\n",
    "            updated = false\n",
    "            for key_fn in keys(tree_copy.children)\n",
    "                if key_fn(X_vec[tree_copy.col_id])\n",
    "                    tree_copy = tree_copy.children[key_fn]\n",
    "                    updated = true\n",
    "                    break\n",
    "                end\n",
    "            end\n",
    "            if !updated\n",
    "                println(\"Error occured for $X_vec\")\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        push!(prediction, Y_pred)\n",
    "    end\n",
    "    return prediction\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTree(1, Dict{Function,DecisionTree}(var\"#3#12\"{Float64}(51.35099100554915) => DecisionTree(-1, nothing, 1.0),Base.var\"#64#65\"{var\"#3#12\"{Float64}}(var\"#3#12\"{Float64}(51.35099100554915)) => DecisionTree(-1, nothing, 0.0)), nothing)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = create_decision_tree(X_data, Y_data, max_depth=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JuTools.compute_accuracy(predict(X_data, tree), Y_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
